{'Q1a': 1.9704162943957475, 'Q1b': 2.0519661033950656, 'Q2': [2.666035950804163, 2.1542691579943236, 2.0280931357090246], 'Q3a': [[1, 4, 4], [1, 4, 4, 4]], 'Q3b': [1.3486665878891972, 1.2742363777450096, 1.2430925626627605], 'Q4a': [[1, 5, 4, 4, 4, 4, 3.71995, 3.71995, 3.71995, 3.71995, 3.71995], [1, 5, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'Q4b': [1.2742363777450096, 1.2430925626627605], 'Q5a': [1, 121, 0, 4], 'Q5b': [2384, 168945, 86232, 3615, 0.4702652880062319], 'Q6a': [1, 75, 0, 1, 0, 0, 0, 0, 0], 'Q6b': 0.17054998141954658, 'Q7': [0.13362134303471274, 0.13276868273457632, 0.1438797938456875, 0.14268606942549644, 0.14302713354555108, 0.1, 0.2128248428558026], 'Q8': 1.8164934412791371, 'Q9': [1.742012484444442, 2.052681872005889, 1.452063234864505], 'Q10': ('I decided to use a form of regression imputation method. \nI chose this method since the original method of using the mean of the\nitem average results in rigid imputations, messing with the natural \ncorrelation of our data. Using regression imputation involves taking the\nalready existing regression and adding some noise to the data, emulating the\nnature of the real data points. This allows us to preserve\nthe current trend of our data, making our predictions more fluid and more\nunderstandable. A major caveat/drawbacks overfitting since we are over-emphasizing\nthe current observable trends we see which may not be accurate. \n', 1.5474605751502788)}
